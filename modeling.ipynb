{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbc2511c-5ba4-41f6-b6f3-8c4f448e3a0d",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dfc794-876a-47a7-91c1-8b8eb92fd3c1",
   "metadata": {},
   "source": [
    "In this section, we will fit 3 different models on the dataset to predict the players' next year salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a978d562-9ccb-42a7-a717-91a93e84869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.api as sm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import warnings\n",
    "from functions import mse_func\n",
    "from functions import split_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "397adfc5-cdae-4544-957f-fa8b86c7e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run eda.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67159c4c-5019-4388-a183-58b2be77f079",
   "metadata": {},
   "source": [
    "To fit any type of model, we first want to split our dataframe into X, our independent predictors, and y, our dependent variable. Our predictors are all columns except the column that we want to predict and y will be the singular 'Next_Year_Salary' column. After getting our X and y, we want to split each into training and test sets using an 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "324c8d88-115b-43a7-b612-5dc4ed60bcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = oh_df.drop(columns=['Next_Year_Salary'])\n",
    "y = np.log(oh_df['Next_Year_Salary'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_train_test(X, y, 0.8, 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60291635-2864-4462-89ed-27e2317744c5",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares (OLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0c87fd-4889-4662-8f70-87f920f6815d",
   "metadata": {},
   "source": [
    "The first model we will use to predict the players' next year salary is OLS. This is a linear model with an intercept term, using all features as predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bed19b78-f8b2-40ce-b966-03e10832fc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:       Next_Year_Salary   R-squared:                       0.638\n",
      "Model:                            OLS   Adj. R-squared:                  0.623\n",
      "Method:                 Least Squares   F-statistic:                     44.16\n",
      "Date:                Sun, 14 Dec 2025   Prob (F-statistic):          3.91e-111\n",
      "Time:                        00:08:50   Log-Likelihood:                -621.12\n",
      "No. Observations:                 601   AIC:                             1290.\n",
      "Df Residuals:                     577   BIC:                             1396.\n",
      "Df Model:                          23                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===============================================================================\n",
      "                  coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          12.0871      0.550     21.976      0.000      11.007      13.167\n",
      "Age             0.0331      0.007      4.740      0.000       0.019       0.047\n",
      "MP_x            0.0696      0.006     11.313      0.000       0.057       0.082\n",
      "PF              0.0757      0.068      1.112      0.266      -0.058       0.209\n",
      "TS%            -2.4860      0.801     -3.104      0.002      -4.059      -0.913\n",
      "TRB%           -0.0210      0.011     -1.824      0.069      -0.044       0.002\n",
      "AST%           -0.0103      0.008     -1.338      0.181      -0.025       0.005\n",
      "STL%           -0.1129      0.055     -2.061      0.040      -0.221      -0.005\n",
      "BLK%           -0.0211      0.026     -0.806      0.420      -0.073       0.030\n",
      "TOV%            0.0174      0.011      1.628      0.104      -0.004       0.038\n",
      "USG%            0.0325      0.008      4.220      0.000       0.017       0.048\n",
      "BPM             0.1159      0.026      4.545      0.000       0.066       0.166\n",
      "NumOfAwards    -0.0481      0.108     -0.445      0.657      -0.260       0.164\n",
      "All-Star        0.0886      0.208      0.427      0.670      -0.319       0.496\n",
      "AwardWinner    -0.0525      0.257     -0.204      0.838      -0.557       0.453\n",
      "FirstTeam      -0.1505      0.434     -0.347      0.729      -1.003       0.702\n",
      "SecondTeam      0.1677      0.394      0.426      0.670      -0.605       0.941\n",
      "ThirdTeam       0.1799      0.304      0.591      0.555      -0.418       0.777\n",
      "DefTeam1        0.0729      0.333      0.219      0.827      -0.581       0.727\n",
      "DefTeam2       -0.0839      0.293     -0.287      0.775      -0.659       0.491\n",
      "Pos_C           2.5285      0.165     15.319      0.000       2.204       2.853\n",
      "Pos_PF          2.5038      0.121     20.635      0.000       2.266       2.742\n",
      "Pos_PG          2.2634      0.143     15.809      0.000       1.982       2.545\n",
      "Pos_SF          2.4312      0.121     20.119      0.000       2.194       2.669\n",
      "Pos_SG          2.3601      0.118     19.919      0.000       2.127       2.593\n",
      "==============================================================================\n",
      "Omnibus:                      107.058   Durbin-Watson:                   2.021\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              280.244\n",
      "Skew:                          -0.898   Prob(JB):                     1.40e-61\n",
      "Kurtosis:                       5.822   Cond. No.                     3.47e+17\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 1.06e-29. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "X_new = sm.add_constant(X_train)\n",
    "ols_model = sm.OLS(y_train, X_new).fit()\n",
    "print(ols_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc2f1fd-2074-4b36-9274-227c64da41e9",
   "metadata": {},
   "source": [
    "Now that we have fit our OLS model, we want to use this model to predict the players' next year salary for our test set. We also want to calculate the mean squared error for future comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bdf8890-1cce-4398-8320-99001f3ab97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7642872784423457\n"
     ]
    }
   ],
   "source": [
    "X_test_new = sm.add_constant(X_test)\n",
    "ols_predictions = ols_model.predict(X_test_new)\n",
    "ols_mse = mse_func(y_test, ols_predictions)\n",
    "print(ols_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97052f2-f72b-4b57-ab45-05310c667f78",
   "metadata": {},
   "source": [
    "Since the MSE is only meaningful relative to other models' MSE, we will plot the residuals to see visually see how the OLS model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9437e75-93c0-4cdd-babd-1a72cb94ef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_residuals = y_test - ols_predictions\n",
    "plt.scatter(ols_predictions, ols_residuals)\n",
    "plt.axhline(0, color='r', linestyle='--')\n",
    "plt.title('Residual Plot for OLS')\n",
    "plt.xlabel('Salary Predictions for Next Year')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n",
    "plt.savefig(\"outputs/OLS_residual_plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78efc52e-4d1e-4443-a6ae-60c0ed8fbc08",
   "metadata": {},
   "source": [
    "Looking at the residual plot above, we see that the residuals are not entirely randomly scattered but are generally within the [-2, 2] range, which suggests there may be a better model for this dataset. There is a slight linear pattern of residuals that can be seen so a nonlinear model may be a better fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d6c5c9-d0fb-403e-a90f-f00c9d5cc141",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329587ad-4c19-47cf-90d0-67779f0836f1",
   "metadata": {},
   "source": [
    "One downside of the OLS model is that it may overfit, especially when noise or multicollinearity exists in the dataset. To prevent overfitting, we will now use ridge regression to regularize the model by limiting the model's complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1b20c5-0442-4a82-8134-b8f8489f5ac2",
   "metadata": {},
   "source": [
    "Before actually fitting the model, we will first standardize the predictors since the predictors all have different units. This prevents predictors with very large or very small units from skewing the model. We apply this standardization to both the training and test set for X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b18ba6-5fef-4370-b152-7552db68b70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = preprocessing.StandardScaler()\n",
    "X_train_ss = ss.fit_transform(X_train)\n",
    "X_test_ss = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc97d8c-0e9e-40c7-9e2b-94fe7353343e",
   "metadata": {},
   "source": [
    "After standardization, we will now use ridge regression combined with 5-fold cross validation to find the best lambda/model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cbaa9f-e240-4eb9-96e8-0bb0fa8ecbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "ridge_model = RidgeCV(alphas=lambdas, cv=5)  \n",
    "ridge_model.fit(X_train_ss, y_train)\n",
    "ridge_pred = ridge_model.predict(X_test_ss)\n",
    "ridge_mse = mse_func(y_test, ridge_pred)\n",
    "print(\"Ridge MSE: \", ridge_mse)\n",
    "print(\"Best lambda for Ridge:\", ridge_model.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461aa2ec-8ee1-4a47-8be7-47123531951c",
   "metadata": {},
   "source": [
    "We see that the MSE for ridge is around 0.615, which is slightly lower than the MSE for OLS. Since the difference seems almost negligible, OLS and ridge regression are pretty similar in terms of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697c099f-8e3c-4ade-a7f6-8f55a260ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_residuals = y_test - ridge_pred\n",
    "plt.scatter(ridge_pred, ridge_residuals)\n",
    "plt.axhline(0, color='r', linestyle='--')\n",
    "plt.title('Residual Plot for Ridge')\n",
    "plt.xlabel('Salary Predictions for Next Year')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n",
    "plt.savefig(\"outputs/Ridge_residual_plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c01a6b-b5db-4caf-930a-4589a81f7e0f",
   "metadata": {},
   "source": [
    "Looking at the residual plot above, we see that it resembles the residual plot for OLS, where there the residuals between [14,16] are not randomly scattered. Therefore, a nonlinear model may be a better fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5241968-35b0-48c1-9720-abb65e7f17e0",
   "metadata": {},
   "source": [
    "## LASSO Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18f432b-0654-4f8c-b78f-1f78ae31f061",
   "metadata": {},
   "source": [
    "Compared to ridge regression, LASSO will only retain features with great prediction power by shrinking the coefficients of irrelevant features to zero. This results in a model that is simpler in complexity, which additionally prevents overfitting. Since the residuals plots for OLS and ridge suggested that a nonlinear model may be better suited for this dataset, we will then use a random forest model to model the nonlinear relationship between the features selected by LASSO and salary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca72171-16a7-4ddc-aee4-4b65830dcce8",
   "metadata": {},
   "source": [
    "Like ridge, we will also use 5-fold cross validation to find the best lambda/model on the standardized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6edd14-ec58-49dd-8578-59c352c26c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "lasso_model = LassoCV(cv=5)\n",
    "lasso_model.fit(X_train_ss, y_train)\n",
    "print(\"Best lambda for Lasso:\", lasso_model.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61834f7f-f374-4c19-a2dc-377832f9d2f4",
   "metadata": {},
   "source": [
    "Now that LASSO has peformed feature selection, we will use these selected features to train the random forest model with 70 estimators. Even though increasing the number of trees in the random forest model will decrease the MSE, we don't want to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4624795-9486-4ea1-9836-b968304511a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = SelectFromModel(lasso_model, prefit=True)\n",
    "X_train_selected = selected.transform(X_train_ss)\n",
    "X_test_selected = selected.transform(X_test_ss)\n",
    "print(\"Number of features before feature selection: \", X_train_ss.shape[1])\n",
    "print(\"Number of features selected: \", X_train_selected.shape[1])\n",
    "rf_model = RandomForestRegressor(n_estimators=70, random_state=42)\n",
    "rf_model.fit(X_train_selected, y_train)\n",
    "lasso_pred = rf_model.predict(X_test_selected)\n",
    "lasso_mse = mse_func(y_test, lasso_pred)\n",
    "print(\"LASSO MSE: \", lasso_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1d20d5-93e9-4252-b4d3-007266a380e8",
   "metadata": {},
   "source": [
    "LASSO reduced model complexity by only using 9 out of the 24 features. We also see that the MSE for LASSO + random forests is much lower than the MSE for ridge or OLS, which proves there is probably a nonlinear relationship between the predictors and salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d75ad3-2f0a-4b27-8cc8-ba8c1267b8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_residuals = y_test - lasso_pred\n",
    "plt.scatter(lasso_pred, lasso_residuals)\n",
    "plt.axhline(0, color='r', linestyle='--')\n",
    "plt.title('Residual Plot for LASSO')\n",
    "plt.xlabel('Salary Predictions for Next Year')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n",
    "plt.savefig(\"outputs/LASSO_residual_plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffefa5a1-23df-4224-8eb4-712788bc3563",
   "metadata": {},
   "source": [
    "Comparing this residual plot to the residual plots for OLS or ridge, we see that the residuals in this plot are slightly more randomly scattered. The outliers present in the other residual plots are no longer present in this plot as LASSO removes features that have low predictive power/noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb09e047-8dfe-4f85-8e57-2d500b677dc3",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9fd69a-d574-4ef2-ae54-fca539c00f86",
   "metadata": {},
   "source": [
    "We previously saw that only 9 out of the 24 features we are working with were used to train the random forest model. We will rank the 9 features by their predictive power as the other 15 features were deemed negligible by LASSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b52535a-d8db-46b8-b5a8-b907ed0e5846",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_contribution = pd.Series(rf_model.feature_importances_, index=X_train.columns[selected.get_support()]).sort_values(ascending=False)\n",
    "print(\"Features from highest to lowest predictive power\")\n",
    "print(feature_contribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef1dfb4-306b-4774-88cd-94aeb9983dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_contribution.plot(kind='barh')\n",
    "plt.xlabel(\"Feature Predictive Power\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Feature Contribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72f7b09-8943-45e8-922e-b194755c33f9",
   "metadata": {},
   "source": [
    "Looking at the bar plot above, we see that out of the 9 features above, the players' minutes played per game was the most predictive of their next year's salary. Minutes played per game contributed to about 60% of the prediction while BPM and age were the 2nd and 3rd more predictive features, contributing about 11% and 7.5% resepectively. LASSO ignores the other 15 features for its predictions while ridge and OLS take all 24 features into account when predicting salary. Since LASSO + random forest resulted in the lowest MSE, we can conclude that a non-linear model using a subset of all the features to predict next year's salary is the best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2aef95-c23e-413e-9058-9028f7226d72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython - nba",
   "language": "python",
   "name": "nba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
